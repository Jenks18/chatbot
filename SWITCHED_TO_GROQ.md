# üöÄ Complete Switch to Groq API

## ‚úÖ What Changed

I've switched your entire backend to use **Groq API** with the **Llama 3.3 70B** model.

### Files Modified:
1. **`backend/services/groq_service.py`** - New Groq API client
2. **`backend/services/model_router.py`** - Now imports Groq service  
3. **`backend/main.py`** - Health checks use Groq
4. **`backend/.env`** - Configured for Groq

## üîë Get Your FREE Groq API Key

1. Go to: **https://console.groq.com/keys**
2. Sign up (free account, no credit card needed)
3. Click **"Create API Key"**
4. Copy your key (starts with `gsk_`)
5. Add to `backend/.env`:
   ```
   GROQ_API_KEY=gsk_your_actual_key_here
   ```

## üß™ Test Locally

```bash
cd /Users/iannjenga/Desktop/chatbot

# Make sure you have your Groq API key in backend/.env
# Then start the server:
PYTHONPATH=backend DEV_SQLITE=1 \\
  /Users/iannjenga/Desktop/chatbot/venv/bin/uvicorn main:app --host 0.0.0.0 --port 8000
```

Visit: http://localhost:8000/health

Should show:
```json
{
    "status": "healthy",
    "database": "healthy",
    "model_server": "healthy"
}
```

## üåê Deploy to Vercel

Add these to Vercel environment variables:

```
GROQ_API_KEY=gsk_your_key_here
GROQ_MODEL=llama-3.3-70b-versatile
NEXT_PUBLIC_API_URL=/api
DATABASE_URL=postgresql://postgres.zzeycmksnujfdvasxoti:kMOFPkWLvHmRWATc@aws-0-us-west-1.pooler.supabase.com:6543/postgres
OPENFDA_API_KEY=rH2feOTgRtT4WRuooKmDqiHlKuDqmhhJK6GqTeAb
NCBI_API_KEY=5141dbd81188ce3fc0547dbcf18a3fbe9209
API_CACHE_DURATION_DAYS=30
ENABLE_API_CACHING=true
NEXT_PUBLIC_SUPABASE_URL=https://zzeycmksnujfdvasxoti.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inp6ZXljbWtzbnVqZmR2YXN4b3RpIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjIyODUxMTYsImV4cCI6MjA3Nzg2MTExNn0.gX37n0KQK9__8oea55JA1JP-JJhF2wUG18jIeaV81oM
```

Then **redeploy** on Vercel.

## ‚ö° Why Groq?

- **Ultra Fast**: Groq's LPU technology = blazing fast responses
- **Free Tier**: Generous free quota (60 requests/minute)
- **Llama 3.3 70B**: State-of-the-art open model
- **No Balance Issues**: Unlike DeepSeek which ran out of credits

## üìã Model Options

You can change the model by updating `GROQ_MODEL` in `.env`:

- `llama-3.3-70b-versatile` (default) - Best quality
- `llama-3.1-8b-instant` - Fastest responses
- `mixtral-8x7b-32768` - 32K context window
- `gemma2-9b-it` - Google's Gemma model

## ‚úÖ Ready to Go!

Just add your Groq API key and you're all set! üöÄ
